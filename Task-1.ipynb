{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the system as described above has its greatest application in an arrayed configuration of antenna elements', 'the child was carefully wrapped and bound into the cradle by means of a cord', 'the author of a keygen uses a disassembler to look at the raw assembly code', 'a misty ridge uprises from the surge', 'the student association is the voice of the undergraduate student population of the state university of new york at buffalohello sir', \"this is the sprawling complex that is peru's largest producer of silver\", 'the current view is that the chronic inflammation in the distal part of the stomach caused by helicobacter pylori infection results in an increased acid production from the non-infected upper corpus region of the stomach', 'people have been moving back into downtown', 'the lawsonite was contained in a platinum crucible and the counter-weight was a plastic crucible with metal pieces', 'the solute was placed inside a beaker and 5 ml of the solvent was pipetted into a 25 ml glass flask for each trial']\n",
      "\n",
      "\n",
      "['component-whole', 'other', 'instrument-agency', 'other', 'member-collection', 'other', 'cause-effect', 'entity-destination', 'content-container', 'entity-destination']\n",
      "\n",
      "\n",
      "['packaging', '', 'merchandise', 'jolliness', 'computer,', 'fad', 'as', 'matured', 'puppy', 'rain']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "f=open(\"TRAIN_FILE.TXT\", 'r')\n",
    "txt1=f.read().translate(str.maketrans(\"\\t\\r\", \"  \"))\n",
    "txt1 = txt1.lower()\n",
    "\"\".join(txt1.split())\n",
    "txt=txt1.split('\\n')\n",
    "sentence_corpora=[]\n",
    "sentence_labels=[]\n",
    "words=[]\n",
    "for i in range(0, 31984, 4):\n",
    "    txt[i]=txt[i].lstrip('0123456789')\n",
    "    txt[i]=txt[i].replace('\\\"','')\n",
    "    txt[i]=txt[i].replace('.','')\n",
    "    at=str(txt[i].strip())\n",
    "    for elem in at.split(\" \"):\n",
    "        words.append(elem.replace(\"<e1>\",\"\").replace(\"</e1>\", \"\").replace(\"</e2>\", \"\").replace(\"<e2>\", \"\"))\n",
    "    sentence_corpora.append(txt[i].strip().replace(\"<e1>\",\"\").replace(\"</e1>\", \"\").replace(\"</e2>\", \"\").replace(\"<e2>\", \"\"))\n",
    "    sentence_labels.append(txt[i+1].strip().replace(\"(e1,e2)\", \"\").replace(\"(e2,e1)\", \"\"))\n",
    "\n",
    "print(sentence_corpora[0:10])\n",
    "print(\"\\n\")\n",
    "print(sentence_labels[:10])\n",
    "print(\"\\n\")\n",
    "words=list(set(words))\n",
    "print(words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[15463, 16054, 7, 2723, 3734, 14280, 13909, 11668, 7581, 6321, 4021, 9468, 3457, 6318, 7927, 10299]\n",
      "16\n",
      "7996\n"
     ]
    }
   ],
   "source": [
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "print (vocab_to_int[''])\n",
    "\n",
    "sentence_int=[]\n",
    "for sentence in sentence_corpora:\n",
    "    sentence_int.append([vocab_to_int[word] for word in sentence.split()])\n",
    "print(sentence_int[0])\n",
    "print(len(sentence_int[0]))\n",
    "print(len(sentence_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 9, 1, 9, 7, 9, 0, 5, 3, 5]\n",
      "7996\n"
     ]
    }
   ],
   "source": [
    "#Setting Label values for Softmax Classifier\n",
    "label_dict={\"cause-effect\": 0, \n",
    "            \"instrument-agency\": 1, \n",
    "            \"product-producer\": 2, \n",
    "            \"content-container\": 3, \n",
    "            \"entity-origin\": 4, \n",
    "            \"entity-destination\": 5, \n",
    "            \"component-whole\": 6,\n",
    "            \"member-collection\": 7,\n",
    "            \"message-topic\": 8,\n",
    "            \"other\": 9}\n",
    "final_labels=[]\n",
    "for elem in sentence_labels:\n",
    "    final_labels.append(label_dict[elem])\n",
    "print(final_labels[0:10])\n",
    "print(len(final_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 85\n"
     ]
    }
   ],
   "source": [
    "# Max sequnce length I could find was 10\n",
    "#padding remaining spaces with 0\n",
    "review_lens = Counter([len(x) for x in sentence_int])\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))\n",
    "seq_len = 85\n",
    "features = np.zeros((len(sentence_int), seq_len), dtype=int)\n",
    "for i, row in enumerate(sentence_int):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7996, 85)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0, 15463, 16054,     7,\n",
       "         2723,  3734, 14280, 13909, 11668,  7581,  6321,  4021,  9468,\n",
       "         3457,  6318,  7927, 10299],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0, 15463,  8371,\n",
       "        18428, 15382,  1711,  9104,  2945, 10522, 15463, 12588,  9008,\n",
       "         1691,  6318, 16487,  7182],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0, 15463, 11627,\n",
       "         6318, 16487,  5662,  5294, 16487, 12771,  9192,  2313, 20097,\n",
       "        15463, 10531,  3548, 22307],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0, 16487,  5690,  9122,\n",
       "         4911, 15907, 15463, 22265],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0, 15463,  3221, 10145,  8740, 15463,  4776,  6318, 15463,\n",
       "        11123,  3221, 13672,  6318, 15463, 16849,  2694,  6318, 13693,\n",
       "        17003, 20097,  7866, 10969]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.shape(features))\n",
    "features[0:5,0:85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "lstm_size= 256\n",
    "lstm_layers= 2\n",
    "batch_size= 512 # no.of sentences fed in 1 epoch \n",
    "learning_rate=0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 9, 1, 9, 7, 9, 0, 5, 3, 5]\n",
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(7196, 85) \n",
      "Validation set: \t(400, 85) \n",
      "Test set: \t\t(400, 85)\n"
     ]
    }
   ],
   "source": [
    "#train test split\n",
    "split_frac = 0.9\n",
    "split_idx = int(len(features)*0.9)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = final_labels[:split_idx], final_labels[split_idx:]\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_words = len(vocab_to_int)\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embed_size= 256\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed=tf.nn.embedding_lookup(embedding, inputs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Main Network\n",
    "with graph.as_default():\n",
    "    lstm=tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop=tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell=tf.contrib.rnn.MultiRNNCell([drop]*lstm_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# RNN Forward pass\n",
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Validation Accuracy\n",
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), final_labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Batching\n",
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 9, 1, 9, 7, 9, 0, 5, 3, 5, 7, 9, 8, 0, 1, 8, 1, 2, 6, 7, 4, 7, 0, 9, 7, 9, 0, 8, 8, 6, 8, 0, 2, 5, 6, 4, 9, 6, 0, 1, 0, 3, 7, 4, 0, 1, 1, 0, 9, 9, 0, 6, 0, 0, 7, 8, 9, 8, 3, 9, 5, 3, 7, 5, 9, 9, 4, 8, 5, 4, 0, 7, 0, 5, 4, 0, 8, 2, 0, 0, 1, 7, 0, 7, 2, 9, 5, 0, 0, 9, 4, 6, 4, 1, 4, 1, 0, 7, 0, 0, 9, 5, 0, 1, 0, 3, 6, 1, 5, 3, 3, 4, 8, 4, 2, 9, 0, 9, 6, 7, 9, 4, 5, 8, 8, 1, 7, 5, 0, 5, 4, 6, 2, 2, 7, 9, 4, 9, 0, 9, 9, 9, 7, 4, 0, 7, 7, 5, 9, 3, 4, 0, 8, 5, 2, 5, 4, 2, 8, 8, 6, 4, 1, 6, 0, 0, 3, 4, 7, 8, 0, 6, 1, 4, 4, 6, 3, 5, 9, 8, 0, 6, 9, 9, 2, 6, 9, 9, 0, 3, 6, 5, 2, 5, 7, 6, 4, 0, 2, 7, 1, 4, 0, 9, 3, 0, 7, 2, 5, 5, 9, 6, 7, 6, 0, 4, 4, 2, 8, 1, 9, 7, 0, 9, 9, 6, 6, 1, 7, 5, 3, 6, 5, 4, 8, 2, 9, 1, 5, 3, 8, 9, 7, 9, 3, 5, 9, 3, 8, 9, 8, 7, 5, 1, 8, 7, 5, 5, 6, 1, 7, 5, 6, 8, 9, 9, 3, 3, 0, 8, 7, 6, 2, 9, 0, 9, 5, 4, 9, 5, 9, 5, 0, 9, 5, 0, 1, 9, 3, 7, 5, 5, 9, 9, 3, 0, 3, 6, 5, 5, 9, 4, 6, 3, 5, 1, 6, 3, 6, 8, 4, 3, 7, 8, 6, 2, 2, 4, 5, 0, 4, 8, 5, 6, 6, 4, 6, 8, 8, 4, 9, 0, 7, 8, 0, 2, 3, 7, 9, 9, 3, 7, 3, 0, 0, 6, 0, 0, 1, 6, 9, 0, 0, 2, 5, 2, 8, 9, 0, 6, 4, 6, 5, 3, 5, 6, 6, 3, 5, 4, 0, 4, 3, 2, 6, 0, 7, 6, 3, 0, 0, 8, 6, 7, 5, 6, 8, 3, 9, 5, 0, 2, 8, 4, 9, 7, 4, 8, 5, 0, 5, 2, 5, 7, 7, 6, 6, 4, 9, 9, 6, 9, 1, 3, 9, 2, 4, 5, 4, 7, 0, 1, 3, 8, 4, 9, 8, 0, 0, 9, 8, 0, 9, 9, 2, 9, 2, 5, 1, 6, 9, 2, 0, 4, 7, 3, 4, 0, 9, 9, 1, 6, 0, 3, 9, 1, 9, 7, 2, 5, 8, 9, 0, 1, 6, 6, 8, 6, 5, 9, 1, 4, 1, 6, 2, 6, 4, 2, 6, 2, 1, 8, 7, 0, 6, 9, 6, 1, 1, 3, 7, 3, 9, 2, 9, 1, 3, 5, 6, 0, 7, 7, 8, 9, 7, 9, 1, 0, 1, 5, 5, 7]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (512,) for Tensor 'labels:0', which has shape '(?, ?)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-5c449adfe5e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     initial_state: state}\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    973\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    976\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (512,) for Tensor 'labels:0', which has shape '(?, ?)'"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "epochs = 10\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            print(y)\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y,\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
