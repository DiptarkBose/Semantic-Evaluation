{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def preprocess(f_name):\n",
    "    f=open(f_name, 'r')\n",
    "    txt1=f.read().translate(str.maketrans(\"\\t\\r\", \"  \"))\n",
    "    #txt1 = txt1.lower()\n",
    "    \"\".join(txt1.split())\n",
    "    txt=txt1.split('\\n')\n",
    "    sentence_corpora=[]\n",
    "    sentence_labels=[]\n",
    "    words=[]\n",
    "    for i in range(0, 32000, 4):\n",
    "        txt[i]=txt[i].lstrip('0123456789')\n",
    "        txt[i]=txt[i].replace('\\\"','')\n",
    "        txt[i]=txt[i].replace('.','')\n",
    "        at=str(txt[i].strip())\n",
    "        for elem in at.split(\" \"):\n",
    "            words.append(elem.replace(\"<e1>\",\"\").replace(\"</e1>\", \"\").replace(\"</e2>\", \"\").replace(\"<e2>\", \"\").lower())\n",
    "        sentence_corpora.append(str(txt[i].strip().replace(\"<e1>\",\"\").replace(\"</e1>\", \"\").replace(\"</e2>\", \"\").replace(\"<e2>\", \"\").lower()))\n",
    "        sentence_labels.append(str(txt[i+1].strip().replace(\"(e1,e2)\", \"\").replace(\"(e2,e1)\", \"\")))\n",
    "    return sentence_corpora,sentence_labels,words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['the system as described above has its greatest application in an arrayed configuration of antenna elements', 'the child was carefully wrapped and bound into the cradle by means of a cord', 'the author of a keygen uses a disassembler to look at the raw assembly code', 'a misty ridge uprises from the surge', 'the student association is the voice of the undergraduate student population of the state university of new york at buffalohello sir', \"this is the sprawling complex that is peru's largest producer of silver\", 'the current view is that the chronic inflammation in the distal part of the stomach caused by helicobacter pylori infection results in an increased acid production from the non-infected upper corpus region of the stomach', 'people have been moving back into downtown', 'the lawsonite was contained in a platinum crucible and the counter-weight was a plastic crucible with metal pieces', 'the solute was placed inside a beaker and 5 ml of the solvent was pipetted into a 25 ml glass flask for each trial']\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "sentence_corpora,sentence_labels,words = preprocess(\"TRAIN_FILE.TXT\")\n",
    "print(type(sentence_corpora))\n",
    "print(sentence_corpora[:10])\n",
    "print(len(sentence_corpora))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22954\n"
     ]
    }
   ],
   "source": [
    "#Setting Label values for Softmax Classifier\n",
    "label_dict={\"Cause-Effect\": 0, \n",
    "            \"Instrument-Agency\": 1, \n",
    "            \"Product-Producer\": 2, \n",
    "            \"Content-Container\": 3, \n",
    "            \"Entity-Origin\": 4, \n",
    "            \"Entity-Destination\": 5, \n",
    "            \"Component-Whole\": 6,\n",
    "            \"Member-Collection\": 7,\n",
    "            \"Message-Topic\": 8,\n",
    "            \"Other\": 9}\n",
    "final_labels=[]\n",
    "for elem in sentence_labels:\n",
    "    final_labels.append(label_dict[elem])\n",
    "final_labels = np.array(final_labels)\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab,0)}\n",
    "print(len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[6 9 1 9 7 9 0 5 3 5]\n"
     ]
    }
   ],
   "source": [
    "print(type(final_labels))\n",
    "print(final_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn's Text Processing functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 19149)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(sentence_corpora)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 19149)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 17244)\t0.0515326448649\n",
      "  (0, 16941)\t0.227637729439\n",
      "  (0, 1394)\t0.150018436856\n",
      "  (0, 5004)\t0.287616091385\n",
      "  (0, 484)\t0.275086385192\n",
      "  (0, 8043)\t0.149812500788\n",
      "  (0, 9253)\t0.196410554269\n",
      "  (0, 7738)\t0.302849612393\n",
      "  (0, 1222)\t0.314925513613\n",
      "  (0, 8729)\t0.0900085469942\n",
      "  (0, 1032)\t0.142652915518\n",
      "  (0, 1354)\t0.390137396568\n",
      "  (0, 3933)\t0.344021441672\n",
      "  (0, 11852)\t0.0726385312515\n",
      "  (0, 1131)\t0.318577951128\n",
      "  (0, 5855)\t0.327001414834\n",
      "  (1, 17244)\t0.11751280552\n",
      "  (1, 11852)\t0.0828208761514\n",
      "  (1, 3279)\t0.313647523517\n",
      "  (1, 18625)\t0.135351638532\n",
      "  (1, 2921)\t0.359071232726\n",
      "  (1, 19005)\t0.36779727957\n",
      "  (1, 1059)\t0.0997817413509\n",
      "  (1, 2393)\t0.392245778146\n",
      "  (1, 9109)\t0.151018297659\n",
      "  :\t:\n",
      "  (2, 1491)\t0.176157517664\n",
      "  (2, 13967)\t0.345530351138\n",
      "  (2, 1428)\t0.331409660897\n",
      "  (2, 3591)\t0.323108950532\n",
      "  (3, 17244)\t0.0670768196775\n",
      "  (3, 11006)\t0.507817440142\n",
      "  (3, 14601)\t0.507817440142\n",
      "  (3, 18156)\t0.469945090977\n",
      "  (3, 7182)\t0.151743653705\n",
      "  (3, 16779)\t0.485663536066\n",
      "  (4, 17244)\t0.177099340864\n",
      "  (4, 11852)\t0.187224564735\n",
      "  (4, 1491)\t0.13954208758\n",
      "  (4, 16552)\t0.458001515461\n",
      "  (4, 1460)\t0.260196555389\n",
      "  (4, 9212)\t0.0964885507519\n",
      "  (4, 18500)\t0.295569782624\n",
      "  (4, 17988)\t0.302145138976\n",
      "  (4, 13107)\t0.242686904\n",
      "  (4, 16314)\t0.217001853541\n",
      "  (4, 18060)\t0.229962537735\n",
      "  (4, 11556)\t0.170392010154\n",
      "  (4, 19088)\t0.247108509275\n",
      "  (4, 2604)\t0.335190809435\n",
      "  (4, 15695)\t0.295569782624\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train_tfidf))\n",
    "print(X_train_tfidf[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f=open('TEST_FILE_CLEAN.TXT', 'r')\n",
    "txt1=f.read().translate(str.maketrans(\"\\t\\r\", \"  \"))\n",
    "txt1 = txt1.lower()\n",
    "\"\".join(txt1.split())\n",
    "txt=txt1.split('\\n')\n",
    "#print(txt[2716])\n",
    "sentence_test=[]\n",
    "words=[]\n",
    "for i in range(0, 2716):\n",
    "    txt[i]=txt[i].lstrip('0123456789')\n",
    "    txt[i]=txt[i].replace('\\\"','')\n",
    "    txt[i]=txt[i].replace('.','')\n",
    "    at=str(txt[i].strip())\n",
    "    for elem in at.split(\" \"):\n",
    "        words.append(elem.replace(\"<e1>\",\"\").replace(\"</e1>\", \"\").replace(\"</e2>\", \"\").replace(\"<e2>\", \"\"))\n",
    "    sentence_test.append(str(txt[i].strip().replace(\"<e1>\",\"\").replace(\"</e1>\", \"\").replace(\"</e2>\", \"\").replace(\"<e2>\", \"\")))\n",
    "    #labels_test.append(str(txt[i+1].strip().replace(\"(e1,e2)\", \"\").replace(\"(e2,e1)\", \"\")))\n",
    "#print(sentence_test[2715])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f=open('TEST_FILE_KEY.TXT', 'r')\n",
    "txt1=f.read().translate(str.maketrans(\"\\t\\r\", \"  \"))\n",
    "\"\".join(txt1.split())\n",
    "labels_test=[]\n",
    "txt=txt1.split('\\n')\n",
    "#print(txt[0:5])\n",
    "for i in range(0, 2716):\n",
    "    txt[i]=txt[i].lstrip('0123456789')\n",
    "    labels_test.append(str(txt[i].strip()))\n",
    "                       \n",
    "#print(labels_test[0:5])\n",
    "\n",
    "final_labels_test=[]\n",
    "for elem in labels_test:\n",
    "    final_labels_test.append(label_dict[elem])\n",
    "final_labels_test = np.array(final_labels_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Multinomial Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                     ('clf', MultinomialNB(alpha=0.01)),\n",
    "])\n",
    "text_clf = text_clf.fit(sentence_corpora, final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58652430044182624"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = text_clf.predict(sentence_test)\n",
    "np.mean(predicted == final_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GridSearchCV for obtaining optimum values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3,1e-3),\n",
    "              'vect__stop_words':('english',None)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(sentence_corpora, final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01,\n",
       " 'tfidf__use_idf': False,\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__stop_words': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilgamesh/anaconda2/envs/env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                     ('clf-svm', SGDClassifier(loss='squared_hinge', penalty='l2',\n",
    "                                           alpha=2*1e-4, n_iter=800, random_state=100,learning_rate='constant',eta0=0.0009)),\n",
    " ])\n",
    "_ = text_clf_svm.fit(sentence_corpora, final_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing predictions in output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f2 = open('output.txt','w')\n",
    "i=8001\n",
    "pred = text_clf_svm.predict(sentence_test)\n",
    "for p in pred:\n",
    "    for label, val in label_dict.items():\n",
    "        if val == p:\n",
    "            #print(label)\n",
    "            f2.write(str(i))\n",
    "            f2.write(\"\\t\")\n",
    "            f2.write(label)\n",
    "            f2.write(\"\\n\")\n",
    "            i=i+1\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess_test(f_name):\n",
    "    f3 = open(f_name,'r')\n",
    "    data = f3.read()\n",
    "    data = data.split(\"\\n\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "output = preprocess_test('output.txt')\n",
    "test = preprocess_test('TEST_FILE_KEY.TXT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(2716):\n",
    "    if output[i]==test[i]:\n",
    "        count = count+1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6205373573794627\n"
     ]
    }
   ],
   "source": [
    "print(count/2717)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
